!pip install -q keras

import keras

! pip install tensorflow

! pip install matplotlib

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import numpy as np

# Step 1: Data Preparation/ Load
train_dir = r'C:\Users\hp\Downloads\brain tumor\Training'
valid_dir = r'C:\Users\hp\Downloads\brain tumor\validation'

# Data Preprocessing
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

valid_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

valid_generator = valid_datagen.flow_from_directory(
    valid_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)


# Load VGG16 model with pre-trained weights and exclude the top layer
base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add custom layers on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(train_generator.num_classes, activation='softmax')(x)

# Define the model
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])


# Train the model
history = model.fit(
    train_generator,
    epochs=20,
    validation_data=valid_generator
)


# Step 4: Plot Training and Validation Metrics with Best Epoch Marked
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

best_epoch = np.argmax(val_acc) + 1  # Find the epoch with the highest validation accuracy

plt.plot(range(1, len(train_acc) + 1), train_acc, 'b', label='Training accuracy')
plt.plot(range(1, len(val_acc) + 1), val_acc, 'r', label='Validation accuracy')
plt.axvline(x=best_epoch, color='g', linestyle='--', label=f'Best Epoch ({best_epoch})')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.figure()

plt.plot(range(1, len(train_loss) + 1), train_loss, 'b', label='Training loss')
plt.plot(range(1, len(val_loss) + 1), val_loss, 'r', label='Validation loss')
plt.axvline(x=best_epoch, color='g', linestyle='--', label=f'Best Epoch ({best_epoch})')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Step 1: Make Predictions on Validation Data
valid_generator.reset()
Y_pred = model.predict(valid_generator)
y_pred = np.argmax(Y_pred, axis=1)

# Step 2: Compute Confusion Matrix and Classification Report
true_classes = valid_generator.classes
class_labels = list(valid_generator.class_indices.keys())

conf_matrix = confusion_matrix(true_classes, y_pred)
report = classification_report(true_classes, y_pred, target_names=class_labels)

# Step 3: Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.show()

# Print Classification Report
print("Classification Report:\n")
print(report)
